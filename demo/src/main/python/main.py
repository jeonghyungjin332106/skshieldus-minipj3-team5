"""
ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ ìƒì„±ê¸° - ì›¹ ì¸í„°í˜ì´ìŠ¤
Gradioë¥¼ í™œìš©í•œ íŒŒì¼ ì—…ë¡œë“œ ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì‹œìŠ¤í…œ
"""

import os
import tempfile
import shutil
from typing import Optional, Tuple, List
from dataclasses import dataclass
import pdfplumber
import docx
from datetime import datetime
import json
import re
from urllib.parse import urljoin, urlparse

# Web scraping
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Gradio
import gradio as gr
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


@dataclass
class CompanyInfo:
    """íšŒì‚¬ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    name: str
    website_url: str = ""
    talent_philosophy: str = ""
    core_values: str = ""
    company_culture: str = ""
    vision_mission: str = ""


class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def extract_pdf_text(file_path: str) -> str:
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (pdfplumber ì‚¬ìš©)"""
        try:
            text = ""
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                        
                        # í…Œì´ë¸”ì´ ìˆëŠ” ê²½ìš° í…Œì´ë¸” ë‚´ìš©ë„ ì¶”ì¶œ
                        tables = page.extract_tables()
                        for table in tables:
                            for row in table:
                                if row:  # Noneì´ ì•„ë‹Œ í–‰ë§Œ ì²˜ë¦¬
                                    row_text = " | ".join([cell or "" for cell in row])
                                    text += row_text + "\n"
            
            if not text.strip():
                raise ValueError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
            return text
        except Exception as e:
            raise Exception(f"PDF íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    @staticmethod
    def extract_docx_text(file_path: str) -> str:
        """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            raise Exception(f"DOCX íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    @staticmethod
    def extract_text_from_uploaded_file(uploaded_file) -> str:
        """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if uploaded_file is None:
            raise ValueError("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Gradio íŒŒì¼ ê°ì²´ì—ì„œ ê²½ë¡œ ì¶”ì¶œ
        if hasattr(uploaded_file, 'name'):
            file_path = uploaded_file.name
        else:
            file_path = str(uploaded_file)
        
        # Windows ê²½ë¡œ ì •ê·œí™”
        file_path = os.path.normpath(file_path)
        
        print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼: {file_path}")
        print(f"ğŸ“‚ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(file_path)}")
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬
        if not os.path.exists(file_path):
            raise ValueError(f"ì—…ë¡œë“œëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # íŒŒì¼ í™•ì¥ìë¡œ ì²˜ë¦¬ ë°©ë²• ê²°ì •
        _, file_extension = os.path.splitext(file_path)
        file_extension = file_extension.lower()
        
        try:
            if file_extension == '.pdf':
                text = DocumentProcessor.extract_pdf_text(file_path)
            elif file_extension == '.docx':
                text = DocumentProcessor.extract_docx_text(file_path)
            elif file_extension == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}. PDF, DOCX, TXTë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
            
            if not text.strip():
                raise ValueError("íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ë¬¸ì")
            return text
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise e


class SimpleWebCrawler:
    """ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ëŸ¬ (Selenium ëŒ€ì‹  requests ê¸°ë°˜)"""
    
    @staticmethod
    def crawl_company_basic_info(website_url: str) -> str:
        """íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ë³¸ ì •ë³´ í¬ë¡¤ë§"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(website_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for script in soup(["script", "style"]):
                script.extract()
            
            text = soup.get_text()
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
            return text[:5000] if len(text) > 5000 else text
            
        except Exception as e:
            return f"ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"


class ResumeAnalyzer:
    """ì´ë ¥ì„œ ë¶„ì„ê¸°"""
    
    def __init__(self, api_key: str):
        """ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            api_key=api_key,
            model="gpt-4o-mini",
            temperature=0.1,
            max_tokens=500
        )
    
    def extract_company_and_position(self, resume_content: str) -> dict:
        """ì´ë ¥ì„œì—ì„œ ì§€ì› íšŒì‚¬ëª…ê³¼ ì§ë¬´ ì¶”ì¶œ"""
        template = """
ë‹¤ìŒ ì´ë ¥ì„œ ë‚´ìš©ì—ì„œ ì§€ì›í•˜ë ¤ëŠ” íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

ì´ë ¥ì„œ ë‚´ìš©:
{resume_content}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
íšŒì‚¬ëª…: [ì°¾ì€ íšŒì‚¬ëª… ë˜ëŠ” "ì—†ìŒ"]
ì§ë¬´: [ì°¾ì€ ì§ë¬´ëª… ë˜ëŠ” "ì—†ìŒ"]

ì°¾ëŠ” ê¸°ì¤€:
- íšŒì‚¬ëª…: "ì§€ì›íšŒì‚¬", "ì§€ì›ê¸°ì—…", "OOíšŒì‚¬ ì§€ì›", "OO ì…ì‚¬ì§€ì›" ë“±ì˜ í‘œí˜„ ê·¼ì²˜
- ì§ë¬´: "ì§€ì›ì§ë¬´", "í¬ë§ì§ë¬´", "ì§€ì›ë¶„ì•¼", "í¬ì§€ì…˜" ë“±ì˜ í‘œí˜„ ê·¼ì²˜

ëª…ì‹œì ìœ¼ë¡œ ê¸°ì¬ë˜ì§€ ì•Šì€ ê²½ìš° "ì—†ìŒ"ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.llm | StrOutputParser()
            
            result = chain.invoke({"resume_content": resume_content[:3000]})  # í† í° ì œí•œ
            
            # ê²°ê³¼ íŒŒì‹±
            company = "ì—†ìŒ"
            position = "ì—†ìŒ"
            
            for line in result.split('\n'):
                if line.startswith('íšŒì‚¬ëª…:'):
                    company = line.replace('íšŒì‚¬ëª…:', '').strip()
                elif line.startswith('ì§ë¬´:'):
                    position = line.replace('ì§ë¬´:', '').strip()
            
            return {
                "company": company if company != "ì—†ìŒ" else "",
                "position": position if position != "ì—†ìŒ" else ""
            }
            
        except Exception as e:
            print(f"ì´ë ¥ì„œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {"company": "", "position": ""}


class InterviewQuestionGenerator:
    """ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸°"""
    
    def __init__(self, api_key: str, model_name: str = "gpt-4o-mini"):
        """ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            api_key=api_key,
            model=model_name,
            temperature=0.7,
            max_tokens=3000
        )
    
    def _create_prompt_template(self) -> ChatPromptTemplate:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        template = """
ë‹¹ì‹ ì€ {company_name}ì˜ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ì´ë ¥ì„œì™€ íšŒì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 
ì ì ˆí•œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

[ì§€ì› ì •ë³´]
- íšŒì‚¬ëª…: {company_name}
- ì§€ì› ì§ë¬´: {position}
- ì¶”ê°€ ìš”êµ¬ì‚¬í•­: {prompt}

[íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´]
{company_website_info}

[ì§€ì›ì ì´ë ¥ì„œ]
{resume_content}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

## 1. ê¸°ë³¸ ì§ˆë¬¸ (2-3ê°œ)
- ìê¸°ì†Œê°œ, ì§€ì›ë™ê¸° ë“± ê¸°ë³¸ì ì¸ ì§ˆë¬¸

## 2. ê²½í—˜ ê¸°ë°˜ ì§ˆë¬¸ (4-5ê°œ)
- ì´ë ¥ì„œì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸
- STAR ë°©ë²•ë¡ ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸

## 3. íšŒì‚¬ ì í•©ì„± ì§ˆë¬¸ (3-4ê°œ)
- íšŒì‚¬ ë¬¸í™”ì™€ ê°€ì¹˜ê´€ì— ëŒ€í•œ ì´í•´ë„ í™•ì¸
- íšŒì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë§ì¶¤í˜• ì§ˆë¬¸

## 4. ì§ë¬´ ì—­ëŸ‰ ì§ˆë¬¸ (3-4ê°œ)
- í•´ë‹¹ ì§ë¬´ì— í•„ìš”í•œ ì „ë¬¸ ì—­ëŸ‰ í™•ì¸
- ì‹¤ë¬´ ìƒí™©ì„ ê°€ì •í•œ ë¬¸ì œ í•´ê²° ì§ˆë¬¸

## 5. ìƒí™© ëŒ€ì‘ ì§ˆë¬¸ (2-3ê°œ)
- ê°ˆë“± ìƒí™©, íŒ€ì›Œí¬, ë¦¬ë”ì‹­ ë“± ìƒí™©ë³„ ëŒ€ì‘ ëŠ¥ë ¥

ê° ì§ˆë¬¸ì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
- **ì§ˆë¬¸**: ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë‚´ìš©
- **ì˜ë„**: ì´ ì§ˆë¬¸ìœ¼ë¡œ ë¬´ì—‡ì„ í‰ê°€í•˜ê³ ì í•˜ëŠ”ì§€
- **íŒ**: ë©´ì ‘ê´€ì´ ì£¼ì˜ ê¹Šê²Œ ë“¤ì–´ì•¼ í•  ë‹µë³€ í¬ì¸íŠ¸

ì´ 15-18ê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
        
        return ChatPromptTemplate.from_template(template)
    
    def generate_questions(self, company_name: str, position: str, prompt: str, 
                         resume_content: str, company_website_info: str = "") -> str:
        """ë©´ì ‘ ì§ˆë¬¸ ìƒì„±"""
        try:
            prompt_template = self._create_prompt_template()
            chain = prompt_template | self.llm | StrOutputParser()
            
            result = chain.invoke({
                "company_name": company_name,
                "position": position,
                "prompt": prompt,
                "resume_content": resume_content,
                "company_website_info": company_website_info or "íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ì—†ìŒ"
            })
            
            return result
            
        except Exception as e:
            raise Exception(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")


class InterviewQuestionInterface:
    """ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì›¹ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        self.generator = InterviewQuestionGenerator(self.api_key)
        self.crawler = SimpleWebCrawler()
        self.analyzer = ResumeAnalyzer(self.api_key)
    
    def analyze_resume(self, resume_file):
        """ì´ë ¥ì„œ ë¶„ì„ ë° íšŒì‚¬ëª…/ì§ë¬´ ì¶”ì¶œ"""
        if resume_file is None:
            return "", "", "âŒ ì´ë ¥ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        try:
            # ì´ë ¥ì„œ ë‚´ìš© ì¶”ì¶œ
            resume_content = DocumentProcessor.extract_text_from_uploaded_file(resume_file)
            
            if not resume_content.strip():
                return "", "", "âŒ ì´ë ¥ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # íšŒì‚¬ëª…ê³¼ ì§ë¬´ ìë™ ì¶”ì¶œ
            extracted_info = self.analyzer.extract_company_and_position(resume_content)
            
            company = extracted_info.get("company", "")
            position = extracted_info.get("position", "")
            
            # ë¶„ì„ ê²°ê³¼ ë©”ì‹œì§€
            if company and position:
                message = f"âœ… ìë™ ì¶”ì¶œ ì™„ë£Œ!\níšŒì‚¬: {company}\nì§ë¬´: {position}"
            elif company:
                message = f"âš ï¸ íšŒì‚¬ëª…ë§Œ ì°¾ì•˜ìŠµë‹ˆë‹¤: {company}\nì§ë¬´ë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."
            elif position:
                message = f"âš ï¸ ì§ë¬´ë§Œ ì°¾ì•˜ìŠµë‹ˆë‹¤: {position}\níšŒì‚¬ëª…ì„ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."
            else:
                message = "âš ï¸ íšŒì‚¬ëª…ê³¼ ì§ë¬´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            return company, position, message
            
        except Exception as e:
            error_msg = f"âŒ ì´ë ¥ì„œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
            return "", "", error_msg
    
    def process_and_generate(self, company_name: str, position: str, 
                           website_url: str, additional_prompt: str,
                           resume_file, enable_crawling: bool = True) -> str:
        """íŒŒì¼ ì²˜ë¦¬ ë° ì§ˆë¬¸ ìƒì„±"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not company_name.strip():
                return "âŒ íšŒì‚¬ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            if not position.strip():
                return "âŒ ì§€ì› ì§ë¬´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            if resume_file is None:
                return "âŒ ì´ë ¥ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            # ì´ë ¥ì„œ ë‚´ìš© ì¶”ì¶œ
            print("ğŸ“„ ì´ë ¥ì„œ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
            resume_content = DocumentProcessor.extract_text_from_uploaded_file(resume_file)
            
            if not resume_content.strip():
                return "âŒ ì´ë ¥ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘
            company_website_info = ""
            if enable_crawling and website_url.strip():
                print("ğŸŒ íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                company_website_info = self.crawler.crawl_company_basic_info(website_url)
            
            # ì§ˆë¬¸ ìƒì„±
            print("ğŸ¤– ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì¤‘...")
            questions = self.generator.generate_questions(
                company_name=company_name,
                position=position,
                prompt=additional_prompt or "íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­ ì—†ìŒ",
                resume_content=resume_content,
                company_website_info=company_website_info
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            result = f"""# ğŸ¯ {company_name} - {position} ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸

## ğŸ“‹ ìƒì„± ì •ë³´
- **íšŒì‚¬ëª…**: {company_name}
- **ì§€ì› ì§ë¬´**: {position}
- **ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ì›¹ì‚¬ì´íŠ¸ ë¶„ì„**: {'í™œì„±í™”' if enable_crawling and website_url else 'ë¹„í™œì„±í™”'}

---

{questions}

---

## ğŸ’¡ ë©´ì ‘ ì¤€ë¹„ íŒ
1. **STAR ë°©ë²•ë¡  í™œìš©**: ìƒí™©(Situation), ê³¼ì œ(Task), í–‰ë™(Action), ê²°ê³¼(Result)ë¡œ ë‹µë³€ êµ¬ì¡°í™”
2. **êµ¬ì²´ì ì¸ ì‚¬ë¡€ ì¤€ë¹„**: ê° ê²½í—˜ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê²°ê³¼ ì¤€ë¹„
3. **íšŒì‚¬ ì—°êµ¬**: íšŒì‚¬ì˜ ìµœê·¼ ë‰´ìŠ¤, ì‚¬ì—… ë°©í–¥, ë¬¸í™” ë“± ì‚¬ì „ ì¡°ì‚¬
4. **ì§ˆë¬¸ ì¤€ë¹„**: ë©´ì ‘ê´€ì—ê²Œ í•  ì—­ì§ˆë¬¸ 2-3ê°œ ì¤€ë¹„

**ë©´ì ‘ í™”ì´íŒ…! ğŸš€**
"""
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            print(error_msg)
            return error_msg
    
    def create_interface(self):
        """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        with gr.Blocks(title="AI ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸°", theme=gr.themes.Soft()) as demo:
            gr.Markdown("""
            # ğŸ¯ AI ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ ìƒì„±ê¸°
            
            ì´ë ¥ì„œì™€ íšŒì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ë“œë¦½ë‹ˆë‹¤.
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“„ ì´ë ¥ì„œ ì—…ë¡œë“œ")
                    
                    resume_file = gr.File(
                        label="ì´ë ¥ì„œ íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX, TXT ì§€ì›)",
                        file_types=[".pdf", ".docx", ".txt"]
                    )
                    
                    analyze_btn = gr.Button(
                        "ğŸ” ì´ë ¥ì„œ ë¶„ì„í•˜ê¸°", 
                        variant="secondary",
                        size="sm"
                    )
                    
                    analysis_result = gr.Textbox(
                        label="ë¶„ì„ ê²°ê³¼",
                        interactive=False,
                        lines=3,
                        visible=False
                    )
                    
                    gr.Markdown("### ğŸ“ ì§€ì› ì •ë³´")
                    
                    company_name = gr.Textbox(
                        label="ğŸ¢ íšŒì‚¬ëª…",
                        placeholder="ì˜ˆ: ë„¤ì´ë²„, ì‚¼ì„±ì „ì, ì¹´ì¹´ì˜¤... (ì´ë ¥ì„œì—ì„œ ìë™ ì¶”ì¶œ ê°€ëŠ¥)"
                    )
                    
                    position = gr.Textbox(
                        label="ğŸ’¼ ì§€ì› ì§ë¬´",
                        placeholder="ì˜ˆ: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì, ë§ˆì¼€íŒ… ë§¤ë‹ˆì €... (ì´ë ¥ì„œì—ì„œ ìë™ ì¶”ì¶œ ê°€ëŠ¥)"
                    )
                    
                    website_url = gr.Textbox(
                        label="ğŸŒ íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ URL (ì„ íƒì‚¬í•­)",
                        placeholder="https://www.company.com (íšŒì‚¬ ì¸ì¬ìƒ ë¶„ì„ìš©)"
                    )
                    
                    additional_prompt = gr.Textbox(
                        label="ğŸ“Œ ì¶”ê°€ ìš”êµ¬ì‚¬í•­ (ì„ íƒì‚¬í•­)",
                        placeholder="ì˜ˆ: ì‹ ì… ê°œë°œì ë©´ì ‘, ë¦¬ë”ì‹­ ê²½í—˜ ì¤‘ì‹œ...",
                        lines=2
                    )
                    
                    with gr.Accordion("âš™ï¸ ê³ ê¸‰ ì„¤ì •", open=False):
                        enable_crawling = gr.Checkbox(
                            label="ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í™œì„±í™” (íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘)",
                            value=True
                        )
                    
                    generate_btn = gr.Button(
                        "ğŸš€ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±í•˜ê¸°", 
                        variant="primary",
                        size="lg"
                    )
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“‹ ìƒì„±ëœ ë©´ì ‘ ì§ˆë¬¸")
                    
                    output = gr.Markdown(
                        value="ì™¼ìª½ì—ì„œ ì •ë³´ë¥¼ ì…ë ¥í•˜ê³  'ë©´ì ‘ ì§ˆë¬¸ ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
                        height=600
                    )
            
            with gr.Accordion("ğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ", open=False):
                gr.Markdown("""
                ### ğŸ“‹ ì‚¬ìš© ìˆœì„œ
                1. **ì´ë ¥ì„œ ì—…ë¡œë“œ**: PDF, DOCX, TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
                2. **ì´ë ¥ì„œ ë¶„ì„**: 'ğŸ” ì´ë ¥ì„œ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ìœ¼ë¡œ íšŒì‚¬ëª…/ì§ë¬´ ìë™ ì¶”ì¶œ
                3. **ì •ë³´ í™•ì¸/ìˆ˜ì •**: ìë™ ì¶”ì¶œëœ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •
                4. **ì§ˆë¬¸ ìƒì„±**: 'ğŸš€ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­
                
                ### ğŸ’¡ ìë™ ì¶”ì¶œ ê¸°ëŠ¥
                - ì´ë ¥ì„œì—ì„œ "ì§€ì›íšŒì‚¬", "í¬ë§ì§ë¬´" ë“±ì˜ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì¤ë‹ˆë‹¤
                - ì¶”ì¶œë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì§ì ‘ ì…ë ¥í•˜ì‹œë©´ ë©ë‹ˆë‹¤
                - ì¶”ì¶œëœ ì •ë³´ëŠ” ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤
                
                ### ì…ë ¥ ì˜ˆì‹œ
                - **ì´ë ¥ì„œ ë‚´ìš©**: "ì‚¼ì„±ì „ì í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì ì§€ì›"
                - **ìë™ ì¶”ì¶œ**: íšŒì‚¬ëª…(ì‚¼ì„±ì „ì), ì§ë¬´(í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì)
                - **ì›¹ì‚¬ì´íŠ¸**: https://www.samsung.com
                - **ì¶”ê°€ ìš”êµ¬ì‚¬í•­**: ì‹ ì… ê°œë°œì, React ê²½í—˜ ì¤‘ì‹œ
                """)
            
            # ì˜ˆì‹œ ì„¹ì…˜
            with gr.Accordion("ğŸ“ ì´ë ¥ì„œ ì‘ì„± íŒ", open=False):
                gr.Markdown("""
                ### ìë™ ì¶”ì¶œì„ ìœ„í•œ ì´ë ¥ì„œ ì‘ì„± íŒ
                
                **ëª…ì‹œì ìœ¼ë¡œ ê¸°ì¬í•˜ë©´ ë” ì •í™•í•©ë‹ˆë‹¤:**
                - âœ… "ì§€ì›íšŒì‚¬: ë„¤ì´ë²„"
                - âœ… "ì§€ì›ì§ë¬´: ë°±ì—”ë“œ ê°œë°œì"
                - âœ… "í¬ë§í¬ì§€ì…˜: ë§ˆì¼€íŒ… ë§¤ë‹ˆì €"
                
                **ì´ëŸ° í‘œí˜„ë“¤ë„ ì¸ì‹í•©ë‹ˆë‹¤:**
                - "OOíšŒì‚¬ ì…ì‚¬ì§€ì›ì„œ"
                - "OO ì§€ì›ë™ê¸°"
                - "í¬ë§ë¶„ì•¼: ë°ì´í„° ë¶„ì„"
                - "ì§€ì›ë¶„ì•¼: AI ì—”ì§€ë‹ˆì–´"
                """)
            
            # ê¸°ì¡´ ì˜ˆì‹œ ì„¹ì…˜
            with gr.Accordion("ğŸ¯ ìƒì„± ê²°ê³¼ ì˜ˆì‹œ", open=False):
                gr.Markdown("""
                ### ì…ë ¥ ì˜ˆì‹œ
                - **íšŒì‚¬ëª…**: ë„¤ì´ë²„
                - **ì§€ì› ì§ë¬´**: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì
                - **ì›¹ì‚¬ì´íŠ¸**: https://www.navercorp.com
                - **ì¶”ê°€ ìš”êµ¬ì‚¬í•­**: ì‹ ì… ê°œë°œì, React ê²½í—˜ ì¤‘ì‹œ
                - **ì´ë ¥ì„œ**: ë³¸ì¸ì˜ ì´ë ¥ì„œ íŒŒì¼ ì—…ë¡œë“œ
                
                ### ìƒì„±ë˜ëŠ” ì§ˆë¬¸ ìœ í˜•
                1. **ê¸°ë³¸ ì§ˆë¬¸**: ìê¸°ì†Œê°œ, ì§€ì›ë™ê¸°
                2. **ê²½í—˜ ê¸°ë°˜**: í”„ë¡œì íŠ¸ ê²½í—˜, ê¸°ìˆ  ìŠ¤íƒ
                3. **íšŒì‚¬ ì í•©ì„±**: ë„¤ì´ë²„ ë¬¸í™”, ê°€ì¹˜ê´€ ë¶€í•©ë„
                4. **ì§ë¬´ ì—­ëŸ‰**: React ê¸°ìˆ ë ¥, í”„ë¡ íŠ¸ì—”ë“œ ì „ë¬¸ì„±
                5. **ìƒí™© ëŒ€ì‘**: íŒ€ì›Œí¬, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
                """)
            
            # ì´ë²¤íŠ¸ ì—°ê²°
            
            # ì´ë ¥ì„œ ë¶„ì„ ë²„íŠ¼
            analyze_btn.click(
                fn=self.analyze_resume,
                inputs=resume_file,
                outputs=[company_name, position, analysis_result]
            ).then(
                fn=lambda: gr.update(visible=True),
                outputs=analysis_result
            )
            
            # ì§ˆë¬¸ ìƒì„± ë²„íŠ¼
            generate_btn.click(
                fn=self.process_and_generate,
                inputs=[
                    company_name, 
                    position, 
                    website_url, 
                    additional_prompt,
                    resume_file, 
                    enable_crawling
                ],
                outputs=output
            )
            
            # íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ ë¶„ì„ ì˜µì…˜
            def auto_analyze_on_upload(file):
                if file is not None:
                    return gr.update(visible=True), f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.name}\nğŸ” 'ì´ë ¥ì„œ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                return gr.update(visible=False), ""
            
            resume_file.change(
                fn=auto_analyze_on_upload,
                inputs=resume_file,
                outputs=[analyze_btn, analysis_result]
            )
        
        return demo


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("ğŸ¯ AI ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸° ì‹œì‘...")
        
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        interface = InterviewQuestionInterface()
        demo = interface.create_interface()
        
        # ì„œë²„ ì‹¤í–‰
        demo.launch(
            share=False,
            debug=True,
            server_name="127.0.0.1",
            server_port=7860,
            show_api=False
        )
        
    except Exception as e:
        print(f"âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì˜¤ë¥˜: {str(e)}")
        print("ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ ì„¤ì •")
        print("2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install -r requirements.txt")


if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
    required_packages = [
        "gradio",
        "langchain-openai",
        "langchain-core", 
        "pdfplumber",
        "python-docx",
        "beautifulsoup4",
        "requests",
        "python-dotenv"
    ]
    
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
    print("pip install " + " ".join(required_packages))
    print("-" * 60)
    
    main()