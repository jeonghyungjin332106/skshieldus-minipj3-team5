{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d37a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "from textwrap import dedent\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import requests\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07c0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 유틸\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 문서 로딩 & 전처리\n",
    "from langchain.document_loaders import Docx2txtLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 벡터화 (임베딩) / 검색\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# LLM 모델\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "# (만약 Ollama 쓰면)\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 체인 및 에이전트\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "# 외부 API 호출 등 필요 시\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "759e8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "llm_model = ChatOllama(model=\"qwen3:1.7b\",temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89fc671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 청크 개수: 66\n",
      "▶️ AI 답변:\n",
      " 지원자는 디버깅에 대한 기본적인 지식과 BlueJ를 사용한 디버깅 경험이 있음을 보여주고 있습니다. 이는 백엔드 개발자로서 버그를 찾고 해결하는 능력이 중요하기 때문에 강조할 수 있는 기술입니다. 또한, 중단점 설정, 단계별 코드 실행, 변수 검사 등의 디버깅 기능을 소개하고 있으므로 이러한 기능을 통해 코드를 분석하고 수정하는 능력을 갖추고 있다는 것을 강조할 수 있습니다. 또한, 독립형 어플리케이션 및 애플릿을 만들고 관련 문서를 생성하는 경험도 언급되어 있으므로 이러한 능력 또한 백엔드 개발자로서 유용하게 활용할 수 있을 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 2. 문서 로딩 및 분할\n",
    "resume_path = Path(\"./data/tutorial-korean.pdf\")  # sample_resume.pdf 또는 sample_resume.docx 경로 지정\n",
    "\n",
    "# 로더 선택\n",
    "if resume_path.suffix.lower() == \".pdf\":\n",
    "    loader = PyPDFLoader(str(resume_path))\n",
    "else:\n",
    "    loader = Docx2txtLoader(str(resume_path))\n",
    "\n",
    "# 로딩 및 분할\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"분할된 청크 개수: {len(chunks)}\")\n",
    "\n",
    "# 3. 임베딩 및 벡터스토어 구축\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# 4. RetrievalQA 체인 생성\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"아래는 지원자 이력서에서 추출된 정보입니다:\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"위 정보를 참고해, '{question}'에 대해 답변해 주세요.\"\n",
    "    )\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "# 5. 예시 질의\n",
    "question = \"백엔드 개발자 포지션에 지원할 때 강조하면 좋을 기술은 무엇일까?\"\n",
    "answer = qa_chain.run(question)\n",
    "print(\"▶️ AI 답변:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4265f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StateGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 2) 그래프 구성\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m graph = \u001b[43mStateGraph\u001b[49m()\n\u001b[32m     33\u001b[39m graph.add_node(START)\n\u001b[32m     34\u001b[39m graph.add_node(\u001b[33m\"\u001b[39m\u001b[33mparse\u001b[39m\u001b[33m\"\u001b[39m, handler=parse_resume)\n",
      "\u001b[31mNameError\u001b[39m: name 'StateGraph' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-ayevwcyA-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
