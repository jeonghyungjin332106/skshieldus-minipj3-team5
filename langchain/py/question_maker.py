"""
ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ ìƒì„±ê¸° - ì›¹ ì¸í„°í˜ì´ìŠ¤
Gradioë¥¼ í™œìš©í•œ íŒŒì¼ ì—…ë¡œë“œ ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì‹œìŠ¤í…œ
"""

import os
import tempfile
import shutil
from typing import Optional, Tuple, List
from dataclasses import dataclass
import pdfplumber
import docx
from datetime import datetime
import json
import re
from urllib.parse import urljoin, urlparse

# Web scraping
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# LangChain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Gradio
import gradio as gr
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


@dataclass
class CompanyInfo:
    """íšŒì‚¬ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    name: str
    website_url: str = ""
    talent_philosophy: str = ""
    core_values: str = ""
    company_culture: str = ""
    vision_mission: str = ""


class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def extract_pdf_text(file_path: str) -> str:
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (pdfplumber ì‚¬ìš©)"""
        try:
            text = ""
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                        
                        # í…Œì´ë¸”ì´ ìˆëŠ” ê²½ìš° í…Œì´ë¸” ë‚´ìš©ë„ ì¶”ì¶œ
                        tables = page.extract_tables()
                        for table in tables:
                            for row in table:
                                if row:  # Noneì´ ì•„ë‹Œ í–‰ë§Œ ì²˜ë¦¬
                                    row_text = " | ".join([cell or "" for cell in row])
                                    text += row_text + "\n"
            
            if not text.strip():
                raise ValueError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
            return text
        except Exception as e:
            raise Exception(f"PDF íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    @staticmethod
    def extract_docx_text(file_path: str) -> str:
        """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            raise Exception(f"DOCX íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    @staticmethod
    def extract_text_from_uploaded_file(uploaded_file) -> str:
        """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if uploaded_file is None:
            raise ValueError("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Gradio íŒŒì¼ ê°ì²´ì—ì„œ ê²½ë¡œ ì¶”ì¶œ
        if hasattr(uploaded_file, 'name'):
            file_path = uploaded_file.name
        else:
            file_path = str(uploaded_file)
        
        # Windows ê²½ë¡œ ì •ê·œí™”
        file_path = os.path.normpath(file_path)
        
        print(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼: {file_path}")
        print(f"ğŸ“‚ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(file_path)}")
        
        # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬
        if not os.path.exists(file_path):
            raise ValueError(f"ì—…ë¡œë“œëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # íŒŒì¼ í™•ì¥ìë¡œ ì²˜ë¦¬ ë°©ë²• ê²°ì •
        _, file_extension = os.path.splitext(file_path)
        file_extension = file_extension.lower()
        
        try:
            if file_extension == '.pdf':
                text = DocumentProcessor.extract_pdf_text(file_path)
            elif file_extension == '.docx':
                text = DocumentProcessor.extract_docx_text(file_path)
            elif file_extension == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}. PDF, DOCX, TXTë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
            
            if not text.strip():
                raise ValueError("íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ë¬¸ì")
            return text
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise e


class SimpleWebCrawler:
    """ê°„ë‹¨í•œ ì›¹ í¬ë¡¤ëŸ¬ (Selenium ëŒ€ì‹  requests ê¸°ë°˜)"""
    
    @staticmethod
    def crawl_company_basic_info(website_url: str) -> str:
        """íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ë³¸ ì •ë³´ í¬ë¡¤ë§"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(website_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for script in soup(["script", "style"]):
                script.extract()
            
            text = soup.get_text()
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
            return text[:5000] if len(text) > 5000 else text
            
        except Exception as e:
            return f"ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"


class ResumeAnalyzer:
    """ì´ë ¥ì„œ ë¶„ì„ê¸°"""
    
    def __init__(self, api_key: str):
        """ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            api_key=api_key,
            model="gpt-4o-mini",
            temperature=0.1,
            max_tokens=500
        )
    
    def extract_company_and_position(self, resume_content: str) -> dict:
        """ì´ë ¥ì„œì—ì„œ ì§€ì› íšŒì‚¬ëª…ê³¼ ì§ë¬´ ì¶”ì¶œ"""
        template = """
ë‹¤ìŒ ì´ë ¥ì„œ ë‚´ìš©ì—ì„œ ì§€ì›í•˜ë ¤ëŠ” íšŒì‚¬ëª…, ì§ë¬´, ê·¸ë¦¬ê³  ì‹ ì…/ê²½ë ¥ ì—¬ë¶€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

ì´ë ¥ì„œ ë‚´ìš©:
{resume_content}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
íšŒì‚¬ëª…: [ì°¾ì€ íšŒì‚¬ëª… ë˜ëŠ” "ì—†ìŒ"]
ì§ë¬´: [ì°¾ì€ ì§ë¬´ëª… ë˜ëŠ” "ì—†ìŒ"]
ê²½ë ¥êµ¬ë¶„: [ì‹ ì…/ê²½ë ¥/ì—†ìŒ]

ì°¾ëŠ” ê¸°ì¤€:
- íšŒì‚¬ëª…: "ì§€ì›íšŒì‚¬", "ì§€ì›ê¸°ì—…", "OOíšŒì‚¬ ì§€ì›", "OO ì…ì‚¬ì§€ì›" ë“±ì˜ í‘œí˜„ ê·¼ì²˜
- ì§ë¬´: "ì§€ì›ì§ë¬´", "í¬ë§ì§ë¬´", "ì§€ì›ë¶„ì•¼", "í¬ì§€ì…˜" ë“±ì˜ í‘œí˜„ ê·¼ì²˜
- ê²½ë ¥êµ¬ë¶„: ê²½ë ¥ì‚¬í•­, ê·¼ë¬´ê²½í—˜ì´ ìˆìœ¼ë©´ "ê²½ë ¥", ì—†ê±°ë‚˜ ì‹ ì… ëª…ì‹œë˜ë©´ "ì‹ ì…", íŒë‹¨ ë¶ˆê°€í•˜ë©´ "ì—†ìŒ"

ëª…ì‹œì ìœ¼ë¡œ ê¸°ì¬ë˜ì§€ ì•Šì€ ê²½ìš° "ì—†ìŒ"ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.llm | StrOutputParser()
            
            result = chain.invoke({"resume_content": resume_content[:3000]})  # í† í° ì œí•œ
            
            # ê²°ê³¼ íŒŒì‹±
            company = "ì—†ìŒ"
            position = "ì—†ìŒ"
            career_level = "ì—†ìŒ"
            
            for line in result.split('\n'):
                if line.startswith('íšŒì‚¬ëª…:'):
                    company = line.replace('íšŒì‚¬ëª…:', '').strip()
                elif line.startswith('ì§ë¬´:'):
                    position = line.replace('ì§ë¬´:', '').strip()
                elif line.startswith('ê²½ë ¥êµ¬ë¶„:'):
                    career_level = line.replace('ê²½ë ¥êµ¬ë¶„:', '').strip()
            
            return {
                "company": company if company != "ì—†ìŒ" else "",
                "position": position if position != "ì—†ìŒ" else "",
                "career_level": career_level if career_level != "ì—†ìŒ" else ""
            }
            
        except Exception as e:
            print(f"ì´ë ¥ì„œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {"company": "", "position": "", "career_level": ""}


class InterviewQuestionGenerator:
    """ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸°"""
    
    def __init__(self, api_key: str, model_name: str = "gpt-4o-mini"):
        """ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            api_key=api_key,
            model=model_name,
            temperature=0.7,
            max_tokens=3000
        )
    
    def _create_prompt_template(self) -> ChatPromptTemplate:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        template = """
ë‹¹ì‹ ì€ {company_name}ì˜ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. {career_level} ì§€ì›ìì˜ ì´ë ¥ì„œì™€ íšŒì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 
ì ì ˆí•œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

[ì§€ì› ì •ë³´]
- íšŒì‚¬ëª…: {company_name}
- ì§€ì› ì§ë¬´: {position}
- ê²½ë ¥ êµ¬ë¶„: {career_level}
- ì¶”ê°€ ìš”êµ¬ì‚¬í•­: {prompt}

[íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´]
{company_website_info}

[ì§€ì›ì ì´ë ¥ì„œ]
{resume_content}

{career_level}ì— ë§ëŠ” ë©´ì ‘ ì§ˆë¬¸ì„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”:

## 1. ê¸°ë³¸ ì§ˆë¬¸ (2-3ê°œ)
{career_basic_guide}

## 2. ê²½í—˜ ê¸°ë°˜ ì§ˆë¬¸ (4-5ê°œ)
{career_experience_guide}

## 3. íšŒì‚¬ ì í•©ì„± ì§ˆë¬¸ (3-4ê°œ)
- íšŒì‚¬ ë¬¸í™”ì™€ ê°€ì¹˜ê´€ì— ëŒ€í•œ ì´í•´ë„ í™•ì¸
- íšŒì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë§ì¶¤í˜• ì§ˆë¬¸

## 4. ì§ë¬´ ì—­ëŸ‰ ì§ˆë¬¸ (3-4ê°œ)
{career_skill_guide}

## 5. ìƒí™© ëŒ€ì‘ ì§ˆë¬¸ (2-3ê°œ)
{career_situation_guide}

ê° ì§ˆë¬¸ì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
- **ì§ˆë¬¸**: ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë‚´ìš©
- **ì˜ë„**: ì´ ì§ˆë¬¸ìœ¼ë¡œ ë¬´ì—‡ì„ í‰ê°€í•˜ê³ ì í•˜ëŠ”ì§€
- **íŒ**: ë©´ì ‘ê´€ì´ ì£¼ì˜ ê¹Šê²Œ ë“¤ì–´ì•¼ í•  ë‹µë³€ í¬ì¸íŠ¸

ì´ 15-18ê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
        
        return ChatPromptTemplate.from_template(template)
    
    def _get_career_level_guides(self, career_level: str) -> dict:
        """ê²½ë ¥ êµ¬ë¶„ì— ë”°ë¥¸ ì§ˆë¬¸ ê°€ì´ë“œ"""
        if career_level == "ì‹ ì…":
            return {
                "career_basic_guide": "- ìê¸°ì†Œê°œ, ì§€ì›ë™ê¸°, í•™ì—…/í”„ë¡œì íŠ¸ ê²½í—˜ ì¤‘ì‹¬ì˜ ê¸°ë³¸ ì§ˆë¬¸",
                "career_experience_guide": "- í•™êµ í”„ë¡œì íŠ¸, ì¸í„´ì‹­, ë™ì•„ë¦¬, ê°œì¸ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì§ˆë¬¸\n- í•™ìŠµ ëŠ¥ë ¥ê³¼ ì„±ì¥ ê°€ëŠ¥ì„±ì— ì´ˆì ",
                "career_skill_guide": "- ê¸°ì´ˆ ì´ë¡  ì§€ì‹ê³¼ í•™ìŠµí•œ ê¸°ìˆ ì— ëŒ€í•œ ì´í•´ë„ í™•ì¸\n- ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±ê³¼ í•™ìŠµ ì˜ì§€ í‰ê°€",
                "career_situation_guide": "- íŒ€ í”„ë¡œì íŠ¸, ê°ˆë“± í•´ê²°, ì‹œê°„ ê´€ë¦¬ ë“± ê¸°ë³¸ì ì¸ ìƒí™© ëŒ€ì‘ ëŠ¥ë ¥\n- í•™ìŠµí•˜ê³  ì„±ì¥í•˜ë ¤ëŠ” ìì„¸ì™€ íƒœë„"
            }
        elif career_level == "ê²½ë ¥":
            return {
                "career_basic_guide": "- ì´ì§ ë™ê¸°, ì»¤ë¦¬ì–´ ëª©í‘œ, í˜„ì¬ê¹Œì§€ì˜ ì„±ê³¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì§ˆë¬¸",
                "career_experience_guide": "- ì‹¤ë¬´ ê²½í—˜ê³¼ êµ¬ì²´ì ì¸ ì„±ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì§ˆë¬¸\n- STAR ë°©ë²•ë¡ ì„ í™œìš©í•œ ìƒì„¸í•œ ê²½í—˜ íƒìƒ‰",
                "career_skill_guide": "- ì „ë¬¸ ê¸°ìˆ  ì—­ëŸ‰ê³¼ ì‹¤ë¬´ ì ìš© ê²½í—˜ í™•ì¸\n- ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ê³¼ ê¸°ìˆ ì  ê¹Šì´ í‰ê°€",
                "career_situation_guide": "- ë¦¬ë”ì‹­, ì˜ì‚¬ê²°ì •, ë³µì¡í•œ ë¬¸ì œ í•´ê²° ë“± ê³ ê¸‰ ìƒí™© ëŒ€ì‘ ëŠ¥ë ¥\n- íŒ€ ê´€ë¦¬, í”„ë¡œì íŠ¸ ë¦¬ë”© ê²½í—˜"
            }
        else:  # êµ¬ë¶„ ì—†ìŒ ë˜ëŠ” ê¸°íƒ€
            return {
                "career_basic_guide": "- ìê¸°ì†Œê°œ, ì§€ì›ë™ê¸° ë“± ê¸°ë³¸ì ì¸ ì§ˆë¬¸",
                "career_experience_guide": "- ì´ë ¥ì„œì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸\n- STAR ë°©ë²•ë¡ ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸",
                "career_skill_guide": "- í•´ë‹¹ ì§ë¬´ì— í•„ìš”í•œ ì „ë¬¸ ì—­ëŸ‰ í™•ì¸\n- ì‹¤ë¬´ ìƒí™©ì„ ê°€ì •í•œ ë¬¸ì œ í•´ê²° ì§ˆë¬¸",
                "career_situation_guide": "- ê°ˆë“± ìƒí™©, íŒ€ì›Œí¬, ë¦¬ë”ì‹­ ë“± ìƒí™©ë³„ ëŒ€ì‘ ëŠ¥ë ¥"
            }
    
    def generate_questions(self, company_name: str, position: str, career_level: str, 
                         prompt: str, resume_content: str, company_website_info: str = "") -> str:
        """ë©´ì ‘ ì§ˆë¬¸ ìƒì„±"""
        try:
            prompt_template = self._create_prompt_template()
            career_guides = self._get_career_level_guides(career_level)
            chain = prompt_template | self.llm | StrOutputParser()
            
            result = chain.invoke({
                "company_name": company_name,
                "position": position,
                "career_level": career_level,
                "prompt": prompt,
                "resume_content": resume_content,
                "company_website_info": company_website_info or "íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ì—†ìŒ",
                **career_guides
            })
            
            return result
            
        except Exception as e:
            raise Exception(f"ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")


class InterviewQuestionInterface:
    """ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì›¹ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        self.generator = InterviewQuestionGenerator(self.api_key)
        self.crawler = SimpleWebCrawler()
        self.analyzer = ResumeAnalyzer(self.api_key)
    
    def analyze_resume(self, resume_file):
        """ì´ë ¥ì„œ ë¶„ì„ ë° íšŒì‚¬ëª…/ì§ë¬´/ê²½ë ¥êµ¬ë¶„ ì¶”ì¶œ"""
        if resume_file is None:
            return "", "", "", "âŒ ì´ë ¥ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        try:
            # ì´ë ¥ì„œ ë‚´ìš© ì¶”ì¶œ
            resume_content = DocumentProcessor.extract_text_from_uploaded_file(resume_file)
            
            if not resume_content.strip():
                return "", "", "", "âŒ ì´ë ¥ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # íšŒì‚¬ëª…, ì§ë¬´, ê²½ë ¥êµ¬ë¶„ ìë™ ì¶”ì¶œ
            extracted_info = self.analyzer.extract_company_and_position(resume_content)
            
            company = extracted_info.get("company", "")
            position = extracted_info.get("position", "")
            career_level = extracted_info.get("career_level", "")
            
            # ë¶„ì„ ê²°ê³¼ ë©”ì‹œì§€
            found_items = []
            if company:
                found_items.append(f"íšŒì‚¬: {company}")
            if position:
                found_items.append(f"ì§ë¬´: {position}")
            if career_level:
                found_items.append(f"ê²½ë ¥êµ¬ë¶„: {career_level}")
            
            if found_items:
                message = f"âœ… ìë™ ì¶”ì¶œ ì™„ë£Œ!\n" + "\n".join(found_items)
                if len(found_items) < 3:
                    message += "\nâš ï¸ ëˆ„ë½ëœ ì •ë³´ëŠ” ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."
            else:
                message = "âš ï¸ íšŒì‚¬ëª…, ì§ë¬´, ê²½ë ¥êµ¬ë¶„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."
            
            return company, position, career_level, message
            
        except Exception as e:
            error_msg = f"âŒ ì´ë ¥ì„œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"
            return "", "", "", error_msg
    
    def save_to_file(self, content: str, company_name: str, position: str, career_level: str) -> str:
        """ê²°ê³¼ë¥¼ data í´ë”ì— txt íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # data í´ë” ìƒì„±
            data_dir = "data"
            os.makedirs(data_dir, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„± (í•œê¸€ íŒŒì¼ëª… ì§€ì›)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            career_suffix = f"_{career_level}" if career_level else ""
            filename = f"ë©´ì ‘ì§ˆë¬¸_{company_name}_{position}{career_suffix}_{timestamp}.txt"
            
            # íŒŒì¼ ê²½ë¡œ
            file_path = os.path.join(data_dir, filename)
            
            # íŒŒì¼ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return file_path
            
        except Exception as e:
            print(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return ""
    
    def process_and_generate(self, company_name: str, position: str, career_level: str,
                           website_url: str, additional_prompt: str,
                           resume_file, enable_crawling: bool = True) -> Tuple[str, str]:
        """íŒŒì¼ ì²˜ë¦¬ ë° ì§ˆë¬¸ ìƒì„±"""
        try:
            # ì…ë ¥ ê²€ì¦
            if not company_name.strip():
                return "âŒ íšŒì‚¬ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""
            
            if not position.strip():
                return "âŒ ì§€ì› ì§ë¬´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""
            
            if resume_file is None:
                return "âŒ ì´ë ¥ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", ""
            
            # ê²½ë ¥ êµ¬ë¶„ ê¸°ë³¸ê°’ ì„¤ì •
            if not career_level.strip():
                career_level = "êµ¬ë¶„ì—†ìŒ"
            
            # ì´ë ¥ì„œ ë‚´ìš© ì¶”ì¶œ
            print("ğŸ“„ ì´ë ¥ì„œ ë‚´ìš© ì¶”ì¶œ ì¤‘...")
            resume_content = DocumentProcessor.extract_text_from_uploaded_file(resume_file)
            
            if not resume_content.strip():
                return "âŒ ì´ë ¥ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", ""
            
            # íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘
            company_website_info = ""
            if enable_crawling and website_url.strip():
                print("ğŸŒ íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                company_website_info = self.crawler.crawl_company_basic_info(website_url)
            
            # ì§ˆë¬¸ ìƒì„±
            print("ğŸ¤– ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì¤‘...")
            questions = self.generator.generate_questions(
                company_name=company_name,
                position=position,
                career_level=career_level,
                prompt=additional_prompt or "íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­ ì—†ìŒ",
                resume_content=resume_content,
                company_website_info=company_website_info
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            result = f"""# ğŸ¯ {company_name} - {position} ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸

## ğŸ“‹ ìƒì„± ì •ë³´
- **íšŒì‚¬ëª…**: {company_name}
- **ì§€ì› ì§ë¬´**: {position}
- **ê²½ë ¥ êµ¬ë¶„**: {career_level}
- **ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ì›¹ì‚¬ì´íŠ¸ ë¶„ì„**: {'í™œì„±í™”' if enable_crawling and website_url else 'ë¹„í™œì„±í™”'}

---

{questions}

---

## ğŸ’¡ ë©´ì ‘ ì¤€ë¹„ íŒ
1. **STAR ë°©ë²•ë¡  í™œìš©**: ìƒí™©(Situation), ê³¼ì œ(Task), í–‰ë™(Action), ê²°ê³¼(Result)ë¡œ ë‹µë³€ êµ¬ì¡°í™”
2. **êµ¬ì²´ì ì¸ ì‚¬ë¡€ ì¤€ë¹„**: ê° ê²½í—˜ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê²°ê³¼ ì¤€ë¹„
3. **íšŒì‚¬ ì—°êµ¬**: íšŒì‚¬ì˜ ìµœê·¼ ë‰´ìŠ¤, ì‚¬ì—… ë°©í–¥, ë¬¸í™” ë“± ì‚¬ì „ ì¡°ì‚¬
4. **ì§ˆë¬¸ ì¤€ë¹„**: ë©´ì ‘ê´€ì—ê²Œ í•  ì—­ì§ˆë¬¸ 2-3ê°œ ì¤€ë¹„

**ë©´ì ‘ í™”ì´íŒ…! ğŸš€**
"""
            
            # íŒŒì¼ ì €ì¥
            saved_path = self.save_to_file(result, company_name, position, career_level)
            download_info = f"\n\nğŸ“ **íŒŒì¼ ì €ì¥ë¨**: `{saved_path}`" if saved_path else ""
            
            return result + download_info, saved_path
            
        except Exception as e:
            error_msg = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            print(error_msg)
            return error_msg, ""
    
    # question_maker.pyì˜ create_interface ë©”ì„œë“œì—ì„œ download_info ë¶€ë¶„ë§Œ ìˆ˜ì •

    def create_interface(self):
        """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        with gr.Blocks(title="AI ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸°", theme=gr.themes.Soft()) as demo:
            gr.Markdown("""
            # ğŸ¯ AI ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ ìƒì„±ê¸°
            
            ì´ë ¥ì„œì™€ íšŒì‚¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ë“œë¦½ë‹ˆë‹¤.
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“„ ì´ë ¥ì„œ ì—…ë¡œë“œ")
                    
                    resume_file = gr.File(
                        label="ì´ë ¥ì„œ íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX, TXT ì§€ì›)",
                        file_types=[".pdf", ".docx", ".txt"]
                    )
                    
                    analyze_btn = gr.Button(
                        "ğŸ” ì´ë ¥ì„œ ë¶„ì„í•˜ê¸°", 
                        variant="secondary",
                        size="sm"
                    )
                    
                    analysis_result = gr.Textbox(
                        label="ë¶„ì„ ê²°ê³¼",
                        interactive=False,
                        lines=3,
                        visible=False
                    )
                    
                    gr.Markdown("### ğŸ“ ì§€ì› ì •ë³´")
                    
                    company_name = gr.Textbox(
                        label="ğŸ¢ íšŒì‚¬ëª…",
                        placeholder="ì˜ˆ: ë„¤ì´ë²„, ì‚¼ì„±ì „ì, ì¹´ì¹´ì˜¤... (ì´ë ¥ì„œì—ì„œ ìë™ ì¶”ì¶œ ê°€ëŠ¥)"
                    )
                    
                    position = gr.Textbox(
                        label="ğŸ’¼ ì§€ì› ì§ë¬´",
                        placeholder="ì˜ˆ: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì, ë§ˆì¼€íŒ… ë§¤ë‹ˆì €... (ì´ë ¥ì„œì—ì„œ ìë™ ì¶”ì¶œ ê°€ëŠ¥)"
                    )
                    
                    career_level = gr.Dropdown(
                        label="ğŸ‘” ê²½ë ¥ êµ¬ë¶„",
                        choices=["ì‹ ì…", "ê²½ë ¥", "êµ¬ë¶„ì—†ìŒ"],
                        value="êµ¬ë¶„ì—†ìŒ"
                    )
                    
                    website_url = gr.Textbox(
                        label="ğŸŒ íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ URL (ì„ íƒì‚¬í•­)",
                        placeholder="https://www.company.com (íšŒì‚¬ ì¸ì¬ìƒ ë¶„ì„ìš©)"
                    )
                    
                    additional_prompt = gr.Textbox(
                        label="ğŸ“Œ ì¶”ê°€ ìš”êµ¬ì‚¬í•­ (ì„ íƒì‚¬í•­)",
                        placeholder="ì˜ˆ: ì‹ ì… ê°œë°œì ë©´ì ‘, ë¦¬ë”ì‹­ ê²½í—˜ ì¤‘ì‹œ...",
                        lines=2
                    )
                    
                    with gr.Accordion("âš™ï¸ ê³ ê¸‰ ì„¤ì •", open=False):
                        enable_crawling = gr.Checkbox(
                            label="ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í™œì„±í™” (íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘)",
                            value=True
                        )
                    
                    generate_btn = gr.Button(
                        "ğŸš€ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±í•˜ê¸°", 
                        variant="primary",
                        size="lg"
                    )
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“‹ ìƒì„±ëœ ë©´ì ‘ ì§ˆë¬¸")
                    
                    output = gr.Markdown(
                        value="ì™¼ìª½ì—ì„œ ì •ë³´ë¥¼ ì…ë ¥í•˜ê³  'ë©´ì ‘ ì§ˆë¬¸ ìƒì„±í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
                        height=600
                    )
                    
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì •ë³´ - ì—¬ê¸°ê°€ í•µì‹¬!
                    download_info = gr.Textbox(
                        label="ğŸ“ íŒŒì¼ ì €ì¥ ì •ë³´",
                        value="ì§ˆë¬¸ ìƒì„± í›„ data í´ë”ì— ìë™ ì €ì¥ë©ë‹ˆë‹¤.",
                        interactive=False,
                        visible=False
                    )
            
            # ì´ë²¤íŠ¸ ì—°ê²°
            
            # ì´ë ¥ì„œ ë¶„ì„ ë²„íŠ¼
            analyze_btn.click(
                fn=self.analyze_resume,
                inputs=resume_file,
                outputs=[company_name, position, career_level, analysis_result]
            ).then(
                fn=lambda: gr.update(visible=True),
                outputs=analysis_result
            )
            
            # ì§ˆë¬¸ ìƒì„± ë²„íŠ¼  
            def generate_and_show_result(company_name, position, career_level, website_url, additional_prompt, resume_file, enable_crawling):
                result, saved_path = self.process_and_generate(
                    company_name, position, career_level, website_url, additional_prompt, resume_file, enable_crawling
                )
                
                # íŒŒì¼ ì €ì¥ ì •ë³´ ì—…ë°ì´íŠ¸
                if saved_path:
                    download_msg = f"âœ… íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {saved_path}"
                    return result, gr.update(value=download_msg, visible=True)
                else:
                    return result, gr.update(visible=False)
            
            generate_btn.click(
                fn=generate_and_show_result,
                inputs=[
                    company_name, 
                    position,
                    career_level,
                    website_url, 
                    additional_prompt,
                    resume_file, 
                    enable_crawling
                ],
                outputs=[output, download_info]  # ì—¬ê¸°ì„œ download_info ì‚¬ìš©
            )
            
            # íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ ë¶„ì„ ì˜µì…˜
            def auto_analyze_on_upload(file):
                if file is not None:
                    return gr.update(visible=True), f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.name}\nğŸ” 'ì´ë ¥ì„œ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                return gr.update(visible=False), ""
            
            resume_file.change(
                fn=auto_analyze_on_upload,
                inputs=resume_file,
                outputs=[analyze_btn, analysis_result]
            )
        
        return demo


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("ğŸ¯ AI ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸° ì‹œì‘...")
        
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        interface = InterviewQuestionInterface()
        demo = interface.create_interface()
        
        # ì„œë²„ ì‹¤í–‰
        demo.launch(
            share=False,
            debug=True,
            server_name="127.0.0.1",
            server_port=7860,
            show_api=False
        )
        
    except Exception as e:
        print(f"âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì˜¤ë¥˜: {str(e)}")
        print("ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print("1. OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ ì„¤ì •")
        print("2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install -r requirements.txt")


if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
    required_packages = [
        "gradio",
        "langchain-openai",
        "langchain-core", 
        "pdfplumber",
        "python-docx",
        "beautifulsoup4",
        "requests",
        "python-dotenv"
    ]
    
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
    print("pip install " + " ".join(required_packages))
    print("-" * 60)
    
    main()